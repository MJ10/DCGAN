{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 28\n",
    "BATCH_SIZE = 50\n",
    "NUM_ITERATIONS = 3000\n",
    "CONVOLUTION_DEPTH = 32\n",
    "LEARNING_RATE = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data = data.drop('label', axis=1)\n",
    "data = data.as_matrix() / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def display_image(image):\n",
    "    image = image.reshape(IMAGE_SIZE, IMAGE_SIZE)\n",
    "    plot.axis('off')\n",
    "    plot.imshow(image, cmap=matplotlib.cm.binary)\n",
    "    plot.show()\n",
    "    \n",
    "    \n",
    "def get_sample_z(size=(1, 100)):\n",
    "    return np.random.normal(size=size)\n",
    "\n",
    "# Loading next batch\n",
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = data.shape[0]\n",
    "\n",
    "\n",
    "def next_batch(batch_size):\n",
    "    global data\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        data = data[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return data[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, batch_size, reuse=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "#         h0 = tf.layers.dense(inputs=z,\n",
    "#                              units=CONVOLUTION_DEPTH * 2 * 2 * 8)\n",
    "#         h0 = tf.reshape(h0, [batch_size, 2, 2, CONVOLUTION_DEPTH * 8])\n",
    "#         h0 = tf.contrib.layers.batch_norm(h0,\n",
    "#                                           decay=0.9,\n",
    "#                                           scale=True,\n",
    "#                                           is_training=True,\n",
    "#                                           epsilon=1e-5)\n",
    "#         h0 = tf.nn.relu(h0)\n",
    "\n",
    "#         g_w1 = tf.get_variable('g_w1', [5, 5, CONVOLUTION_DEPTH * 4 , CONVOLUTION_DEPTH * 8],\n",
    "#                                dtype=tf.float32,\n",
    "#                                initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "#         g_b1 = tf.get_variable('g_b1', [CONVOLUTION_DEPTH * 4],\n",
    "#                                initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "#         h1 = tf.nn.conv2d_transpose(h0,\n",
    "#                                     g_w1,\n",
    "#                                     strides=[1, 2, 2, 1],\n",
    "#                                     output_shape=[batch_size, 4, 4, CONVOLUTION_DEPTH * 4])\n",
    "#         h1 = h1 + g_b1\n",
    "#         h1 = tf.contrib.layers.batch_norm(h1,\n",
    "#                                           decay=0.9,\n",
    "#                                           scale=True,\n",
    "#                                           is_training=True,\n",
    "#                                           epsilon=1e-5)\n",
    "#         h1 = tf.nn.relu(h1)\n",
    "\n",
    "#         g_w2 = tf.get_variable('g_w2',\n",
    "#                                [5, 5, CONVOLUTION_DEPTH * 2, CONVOLUTION_DEPTH * 4],\n",
    "#                                dtype=tf.float32,\n",
    "#                                initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "#         g_b2 = tf.get_variable('g_b2', [CONVOLUTION_DEPTH * 2],\n",
    "#                                initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "#         h2 = tf.nn.conv2d_transpose(h1,\n",
    "#                                     g_w2,\n",
    "#                                     output_shape=[batch_size, 7, 7, CONVOLUTION_DEPTH * 2],\n",
    "#                                     strides=[1, 2, 2, 1])\n",
    "#         h2 = h2 + g_b2\n",
    "\n",
    "#         h2 = tf.contrib.layers.batch_norm(h2,\n",
    "#                                           decay=0.9,\n",
    "#                                           scale=True,\n",
    "#                                           is_training=True,\n",
    "#                                           epsilon=1e-5)\n",
    "#         h2 = tf.nn.relu(h2)\n",
    "\n",
    "#         g_w3 = tf.get_variable('g_w3',\n",
    "#                                [5, 5, CONVOLUTION_DEPTH * 1, CONVOLUTION_DEPTH * 2],\n",
    "#                                dtype=tf.float32,\n",
    "#                                initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "#         g_b3 = tf.get_variable('g_b3', [CONVOLUTION_DEPTH * 1],\n",
    "#                                initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "#         h3 = tf.nn.conv2d_transpose(h2,\n",
    "#                                     g_w3,\n",
    "#                                     output_shape=[batch_size, 14, 14, CONVOLUTION_DEPTH * 1],\n",
    "#                                     strides=[1, 2, 2, 1])\n",
    "#         h3 = h3 + g_b3\n",
    "\n",
    "#         h3 = tf.contrib.layers.batch_norm(h3,\n",
    "#                                           decay=0.9,\n",
    "#                                           scale=True,\n",
    "#                                           is_training=True,\n",
    "#                                           epsilon=1e-5)\n",
    "#         h3 = tf.nn.relu(h3)\n",
    "\n",
    "#         g_w4 = tf.get_variable('g_w4',\n",
    "#                                [5, 5, 1, CONVOLUTION_DEPTH * 1],\n",
    "#                                dtype=tf.float32,\n",
    "#                                initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "#         g_b4 = tf.get_variable('g_b4', [1],\n",
    "#                                initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "#         h4 = tf.nn.conv2d_transpose(h3,\n",
    "#                                     g_w4,\n",
    "#                                     output_shape=[batch_size, 28, 28, 1],\n",
    "#                                     strides=[1, 2, 2, 1])\n",
    "#         h4 = h4 + g_b4\n",
    "\n",
    "#         return tf.nn.tanh(h4)\n",
    "    g_w1 = tf.get_variable('g_w1', [100, 3136], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b1 = tf.get_variable('g_b1', [3136], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g1 = tf.matmul(z, g_w1) + g_b1\n",
    "    g1 = tf.reshape(g1, [-1, 56, 56, 1])\n",
    "    g1 = tf.contrib.layers.batch_norm(g1, epsilon=1e-5, scope='bn1')\n",
    "    g1 = tf.nn.relu(g1)\n",
    "\n",
    "    # Generate 50 features\n",
    "    g_w2 = tf.get_variable('g_w2', [3, 3, 1, 100/2], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b2 = tf.get_variable('g_b2', [100/2], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g2 = tf.nn.conv2d(g1, g_w2, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    g2 = g2 + g_b2\n",
    "    g2 = tf.contrib.layers.batch_norm(g2, epsilon=1e-5, scope='bn2')\n",
    "    g2 = tf.nn.relu(g2)\n",
    "    g2 = tf.image.resize_images(g2, [56, 56])\n",
    "\n",
    "    # Generate 25 features\n",
    "    g_w3 = tf.get_variable('g_w3', [3, 3, 100/2, 100/4], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b3 = tf.get_variable('g_b3', [100/4], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g3 = tf.nn.conv2d(g2, g_w3, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    g3 = g3 + g_b3\n",
    "    g3 = tf.contrib.layers.batch_norm(g3, epsilon=1e-5, scope='bn3')\n",
    "    g3 = tf.nn.relu(g3)\n",
    "    g3 = tf.image.resize_images(g3, [56, 56])\n",
    "\n",
    "    # Final convolution with one output channel\n",
    "    g_w4 = tf.get_variable('g_w4', [1, 1, 100/4, 1], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b4 = tf.get_variable('g_b4', [1], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g4 = tf.nn.conv2d(g3, g_w4, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    g4 = g4 + g_b4\n",
    "    g4 = tf.sigmoid(g4)\n",
    "\n",
    "    # No batch normalization at the final layer, but we do add\n",
    "    # a sigmoid activator to make the generated images crisper.\n",
    "    # Dimensions of g4: batch_size x 28 x 28 x 1\n",
    "\n",
    "    return g4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(image, reuse=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        image = tf.reshape(image, [-1, 28, 28, 1])\n",
    "        h0 = tf.layers.conv2d(inputs=image,\n",
    "                              kernel_size=[5, 5],\n",
    "                              filters=CONVOLUTION_DEPTH,\n",
    "                              strides=(2, 2),\n",
    "                              padding='SAME')\n",
    "        h0 = tf.maximum(h0, 0.2*h0)\n",
    "\n",
    "        h1 = tf.layers.conv2d(inputs=h0,\n",
    "                              kernel_size=[5, 5],\n",
    "                              filters=2*CONVOLUTION_DEPTH,\n",
    "                              strides=(2, 2),\n",
    "                              padding='SAME')\n",
    "        h1 = tf.contrib.layers.batch_norm(h1,\n",
    "                                          decay=0.9,\n",
    "                                          scale=True,\n",
    "                                          is_training=True,\n",
    "                                          epsilon=1e-5)\n",
    "        h1 = tf.maximum(h1, 0.2*h1)\n",
    "\n",
    "        h2 = tf.layers.conv2d(inputs=h1,\n",
    "                              kernel_size=[5, 5],\n",
    "                              filters=4 * CONVOLUTION_DEPTH,\n",
    "                              strides=(2, 2),\n",
    "                              padding='SAME')\n",
    "        h2 = tf.contrib.layers.batch_norm(h2,\n",
    "                                          decay=0.9,\n",
    "                                          scale=True,\n",
    "                                          is_training=True,\n",
    "                                          epsilon=1e-5)\n",
    "        h2 = tf.maximum(h2, 0.2 * h2)\n",
    "\n",
    "        h3 = tf.layers.conv2d(inputs=h2,\n",
    "                              kernel_size=[5, 5],\n",
    "                              filters=8 * CONVOLUTION_DEPTH,\n",
    "                              strides=(2, 2),\n",
    "                              padding='SAME')\n",
    "        h3 = tf.contrib.layers.batch_norm(h3,\n",
    "                                          decay=0.9,\n",
    "                                          scale=True,\n",
    "                                          is_training=True,\n",
    "                                          epsilon=1e-5)\n",
    "        h3 = tf.maximum(h3, 0.2 * h3)\n",
    "\n",
    "        h4 = tf.layers.dense(inputs=h3,\n",
    "                             units=1)\n",
    "\n",
    "        return tf.nn.sigmoid(h4), h4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_in = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "image_in = tf.placeholder(tf.float32, shape=[None,\n",
    "                                             IMAGE_SIZE * IMAGE_SIZE])\n",
    "\n",
    "gen_sample = generator(Z_in, BATCH_SIZE)\n",
    "\n",
    "discriminator_data, data_logits = discriminator(image_in)\n",
    "discriminator_model, model_logits = discriminator(gen_sample,\n",
    "                                                  reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# losses\n",
    "discriminator_loss_data = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=data_logits,\n",
    "                                                                                  labels=tf.zeros_like(discriminator_data)))\n",
    "\n",
    "discriminator_loss_model = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_logits,\n",
    "                                                                                   labels=tf.ones_like(discriminator_model)))\n",
    "\n",
    "discriminator_loss = discriminator_loss_data + discriminator_loss_model\n",
    "\n",
    "generator_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_logits,\n",
    "                                                                        labels=tf.zeros_like(discriminator_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vars = tf.trainable_variables()\n",
    "generator_vars = [var for var in all_vars if var.name.startswith('generator')]\n",
    "discriminator_vars = [var for var in all_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "discriminator_optimize = tf.train.AdamOptimizer(LEARNING_RATE).minimize(discriminator_loss,\n",
    "                                                                        var_list=discriminator_vars)\n",
    "generator_optimize = tf.train.AdamOptimizer(LEARNING_RATE).minimize(generator_loss,\n",
    "                                                                    var_list=generator_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 => Discriminator: 0.32762134075164795 | Generator: 1.8888195753097534\n",
      "Step 1 => Discriminator: 0.3273317217826843 | Generator: 1.9211628437042236\n",
      "Step 2 => Discriminator: 0.3025984466075897 | Generator: 1.935577392578125\n",
      "Step 3 => Discriminator: 0.29963982105255127 | Generator: 1.9508371353149414\n",
      "Step 4 => Discriminator: 0.30353492498397827 | Generator: 1.977390170097351\n",
      "Step 5 => Discriminator: 0.3106721341609955 | Generator: 1.9854775667190552\n",
      "Step 6 => Discriminator: 0.33881717920303345 | Generator: 2.001387119293213\n",
      "Step 7 => Discriminator: 0.3516162633895874 | Generator: 2.01613712310791\n",
      "Step 8 => Discriminator: 0.31664031744003296 | Generator: 2.0351204872131348\n",
      "Step 9 => Discriminator: 0.32634615898132324 | Generator: 2.06760311126709\n",
      "Step 10 => Discriminator: 0.3034484386444092 | Generator: 2.0849311351776123\n",
      "Step 11 => Discriminator: 0.2706495523452759 | Generator: 2.10007643699646\n",
      "Step 12 => Discriminator: 0.2932876944541931 | Generator: 2.114044189453125\n",
      "Step 13 => Discriminator: 0.2751888930797577 | Generator: 2.104630947113037\n",
      "Step 14 => Discriminator: 0.29725658893585205 | Generator: 2.0968689918518066\n",
      "Step 15 => Discriminator: 0.263186514377594 | Generator: 2.1018173694610596\n",
      "Step 16 => Discriminator: 0.29592740535736084 | Generator: 2.115414619445801\n",
      "Step 17 => Discriminator: 0.27545589208602905 | Generator: 2.1172211170196533\n",
      "Step 18 => Discriminator: 0.29649150371551514 | Generator: 2.1321849822998047\n",
      "Step 19 => Discriminator: 0.24866348505020142 | Generator: 2.150041103363037\n",
      "Step 20 => Discriminator: 0.26118093729019165 | Generator: 2.158303737640381\n",
      "Step 21 => Discriminator: 0.2913339138031006 | Generator: 2.160186529159546\n",
      "Step 22 => Discriminator: 0.26713237166404724 | Generator: 2.1523048877716064\n",
      "Step 23 => Discriminator: 0.24086149036884308 | Generator: 2.1504147052764893\n",
      "Step 24 => Discriminator: 0.25537538528442383 | Generator: 2.148169755935669\n",
      "Step 25 => Discriminator: 0.2527765929698944 | Generator: 2.161813735961914\n",
      "Step 26 => Discriminator: 0.27474337816238403 | Generator: 2.162980794906616\n",
      "Step 27 => Discriminator: 0.2476758062839508 | Generator: 2.1705944538116455\n",
      "Step 28 => Discriminator: 0.24593615531921387 | Generator: 2.1858737468719482\n",
      "Step 29 => Discriminator: 0.2373318374156952 | Generator: 2.1731488704681396\n",
      "Step 30 => Discriminator: 0.24862751364707947 | Generator: 2.175992250442505\n",
      "Step 31 => Discriminator: 0.2414996325969696 | Generator: 2.180511713027954\n",
      "Step 32 => Discriminator: 0.265366792678833 | Generator: 2.174971342086792\n",
      "Step 33 => Discriminator: 0.22949421405792236 | Generator: 2.1800293922424316\n",
      "Step 34 => Discriminator: 0.22557571530342102 | Generator: 2.191526174545288\n",
      "Step 35 => Discriminator: 0.23707690834999084 | Generator: 2.1952273845672607\n",
      "Step 36 => Discriminator: 0.2439427375793457 | Generator: 2.194314479827881\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-000d23d06599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_ITERATIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimage_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator_optimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mZ_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_sample_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_optimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mZ_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_sample_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/moksh/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/moksh/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/moksh/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/moksh/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/moksh/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(NUM_ITERATIONS):\n",
    "    image_batch = next_batch(BATCH_SIZE)\n",
    "    sess.run(discriminator_optimize, feed_dict={Z_in: get_sample_z([BATCH_SIZE, 100]), image_in: image_batch})\n",
    "    sess.run(generator_optimize, feed_dict={Z_in: get_sample_z([BATCH_SIZE, 100])})\n",
    "    \n",
    "    disc_losses = sess.run(discriminator_loss, feed_dict={Z_in: get_sample_z([BATCH_SIZE, 100]),\n",
    "                                                          image_in: image_batch})\n",
    "    gen_losses = sess.run(generator_loss, feed_dict={Z_in: get_sample_z([BATCH_SIZE, 100])})\n",
    "    \n",
    "    if i % 1 == 0:\n",
    "        print('Step {} => Discriminator: {} | Generator: {}'.format(i, disc_losses, gen_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/IBTlB9wivIDTlF+wCnKDzj1L94AdAnVSpSWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9de7594630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_g = generator(Z_in, 1, True)\n",
    "x = sess.run(sample_g, feed_dict={Z_in: get_sample_z([1, 100])})\n",
    "display_image(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
