{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 28\n",
    "BATCH_SIZE = 50\n",
    "NUM_ITERATIONS = 3000\n",
    "CONVOLUTION_DEPTH = 32\n",
    "LEARNING_RATE = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data = data.drop('label', axis=1)\n",
    "data = data.as_matrix() / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def display_image(image):\n",
    "    image = image.reshape(IMAGE_SIZE, IMAGE_SIZE)\n",
    "    plot.axis('off')\n",
    "    plot.imshow(image, cmap=matplotlib.cm.binary)\n",
    "    plot.show()\n",
    "    \n",
    "    \n",
    "def get_sample_z(size=(1, 100)):\n",
    "    return np.random.normal(size=size)\n",
    "\n",
    "# Loading next batch\n",
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = data.shape[0]\n",
    "\n",
    "\n",
    "def next_batch(batch_size):\n",
    "    global data\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        data = data[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return data[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, batch_size, reuse=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        h0 = tf.layers.dense(inputs=z,\n",
    "                             units=CONVOLUTION_DEPTH * 2 * 2 * 8)\n",
    "        h0 = tf.reshape(h0, [batch_size, 2, 2, CONVOLUTION_DEPTH * 8])\n",
    "        h0 = tf.contrib.layers.batch_norm(h0,\n",
    "                                          decay=0.9,\n",
    "                                          scale=True,\n",
    "                                          is_training=True,\n",
    "                                          epsilon=1e-5)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "\n",
    "        g_w1 = tf.get_variable('g_w1', [5, 5, CONVOLUTION_DEPTH * 4 , CONVOLUTION_DEPTH * 8],\n",
    "                               dtype=tf.float32,\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        g_b1 = tf.get_variable('g_b1', [CONVOLUTION_DEPTH * 4],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        h1 = tf.nn.conv2d_transpose(h0,\n",
    "                                    g_w1,\n",
    "                                    strides=[1, 2, 2, 1],\n",
    "                                    output_shape=[batch_size, 4, 4, CONVOLUTION_DEPTH * 4])\n",
    "        h1 = h1 + g_b1\n",
    "        h1 = tf.contrib.layers.batch_norm(h1,\n",
    "                                          decay=0.9,\n",
    "                                          scale=True,\n",
    "                                          is_training=True,\n",
    "                                          epsilon=1e-5)\n",
    "        h1 = tf.nn.relu(h1)\n",
    "\n",
    "        g_w2 = tf.get_variable('g_w2',\n",
    "                               [5, 5, CONVOLUTION_DEPTH * 2, CONVOLUTION_DEPTH * 4],\n",
    "                               dtype=tf.float32,\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        g_b2 = tf.get_variable('g_b2', [CONVOLUTION_DEPTH * 2],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        h2 = tf.nn.conv2d_transpose(h1,\n",
    "                                    g_w2,\n",
    "                                    output_shape=[batch_size, 7, 7, CONVOLUTION_DEPTH * 2],\n",
    "                                    strides=[1, 2, 2, 1])\n",
    "        h2 = h2 + g_b2\n",
    "\n",
    "        h2 = tf.contrib.layers.batch_norm(h2,\n",
    "                                          decay=0.9,\n",
    "                                          scale=True,\n",
    "                                          is_training=True,\n",
    "                                          epsilon=1e-5)\n",
    "        h2 = tf.nn.relu(h2)\n",
    "\n",
    "        g_w3 = tf.get_variable('g_w3',\n",
    "                               [5, 5, CONVOLUTION_DEPTH * 1, CONVOLUTION_DEPTH * 2],\n",
    "                               dtype=tf.float32,\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        g_b3 = tf.get_variable('g_b3', [CONVOLUTION_DEPTH * 1],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        h3 = tf.nn.conv2d_transpose(h2,\n",
    "                                    g_w3,\n",
    "                                    output_shape=[batch_size, 14, 14, CONVOLUTION_DEPTH * 1],\n",
    "                                    strides=[1, 2, 2, 1])\n",
    "        h3 = h3 + g_b3\n",
    "\n",
    "        h3 = tf.contrib.layers.batch_norm(h3,\n",
    "                                          decay=0.9,\n",
    "                                          scale=True,\n",
    "                                          is_training=True,\n",
    "                                          epsilon=1e-5)\n",
    "        h3 = tf.nn.relu(h3)\n",
    "\n",
    "        g_w4 = tf.get_variable('g_w4',\n",
    "                               [5, 5, 1, CONVOLUTION_DEPTH * 1],\n",
    "                               dtype=tf.float32,\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        g_b4 = tf.get_variable('g_b4', [1],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        h4 = tf.nn.conv2d_transpose(h3,\n",
    "                                    g_w4,\n",
    "                                    output_shape=[batch_size, 28, 28, 1],\n",
    "                                    strides=[1, 2, 2, 1])\n",
    "        h4 = h4 + g_b4\n",
    "\n",
    "        return tf.nn.tanh(h4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(image, reuse=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        image = tf.reshape(image, [-1, 28, 28, 1])\n",
    "        h0 = tf.layers.conv2d(inputs=image,\n",
    "                              kernel_size=[5, 5],\n",
    "                              filters=CONVOLUTION_DEPTH,\n",
    "                              strides=(2, 2),\n",
    "                              padding='SAME')\n",
    "        h0 = tf.maximum(h0, 0.2*h0)\n",
    "\n",
    "        h1 = tf.layers.conv2d(inputs=h0,\n",
    "                              kernel_size=[5, 5],\n",
    "                              filters=2*CONVOLUTION_DEPTH,\n",
    "                              strides=(2, 2),\n",
    "                              padding='SAME')\n",
    "        h1 = tf.contrib.layers.batch_norm(h1,\n",
    "                                          decay=0.9,\n",
    "                                          scale=True,\n",
    "                                          is_training=True,\n",
    "                                          epsilon=1e-5)\n",
    "        h1 = tf.maximum(h1, 0.2*h1)\n",
    "\n",
    "        h2 = tf.layers.conv2d(inputs=h1,\n",
    "                              kernel_size=[5, 5],\n",
    "                              filters=4 * CONVOLUTION_DEPTH,\n",
    "                              strides=(2, 2),\n",
    "                              padding='SAME')\n",
    "        h2 = tf.contrib.layers.batch_norm(h2,\n",
    "                                          decay=0.9,\n",
    "                                          scale=True,\n",
    "                                          is_training=True,\n",
    "                                          epsilon=1e-5)\n",
    "        h2 = tf.maximum(h2, 0.2 * h2)\n",
    "\n",
    "        h3 = tf.layers.conv2d(inputs=h2,\n",
    "                              kernel_size=[5, 5],\n",
    "                              filters=8 * CONVOLUTION_DEPTH,\n",
    "                              strides=(2, 2),\n",
    "                              padding='SAME')\n",
    "        h3 = tf.contrib.layers.batch_norm(h3,\n",
    "                                          decay=0.9,\n",
    "                                          scale=True,\n",
    "                                          is_training=True,\n",
    "                                          epsilon=1e-5)\n",
    "        h3 = tf.maximum(h3, 0.2 * h3)\n",
    "\n",
    "        h4 = tf.layers.dense(inputs=h3,\n",
    "                             units=1)\n",
    "\n",
    "        return tf.nn.sigmoid(h4), h4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_in = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "image_in = tf.placeholder(tf.float32, shape=[None,\n",
    "                                             IMAGE_SIZE * IMAGE_SIZE])\n",
    "\n",
    "gen_sample = generator(Z_in, BATCH_SIZE)\n",
    "\n",
    "discriminator_data, data_logits = discriminator(image_in)\n",
    "discriminator_model, model_logits = discriminator(gen_sample,\n",
    "                                                  reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# losses\n",
    "discriminator_loss_data = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=data_logits,\n",
    "                                                                                  labels=tf.ones_like(discriminator_data)))\n",
    "\n",
    "discriminator_loss_model = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_logits,\n",
    "                                                                                   labels=tf.zeros_like(discriminator_model)))\n",
    "\n",
    "discriminator_loss = discriminator_loss_data + discriminator_loss_model\n",
    "\n",
    "generator_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_logits,\n",
    "                                                                        labels=tf.ones_like(discriminator_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vars = tf.trainable_variables()\n",
    "generator_vars = [var for var in all_vars if var.name.startswith('generator')]\n",
    "discriminator_vars = [var for var in all_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "discriminator_optimize = tf.train.AdamOptimizer(LEARNING_RATE).minimize(discriminator_loss,\n",
    "                                                                        var_list=discriminator_vars)\n",
    "generator_optimize = tf.train.AdamOptimizer(LEARNING_RATE).minimize(generator_loss,\n",
    "                                                                    var_list=generator_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_ITERATIONS):\n",
    "    image_batch = next_batch(BATCH_SIZE)\n",
    "    sess.run(discriminator_optimize, feed_dict={Z_in: get_sample_z([BATCH_SIZE, 100]), image_in: image_batch})\n",
    "    sess.run(generator_optimize, feed_dict={Z_in: get_sample_z([BATCH_SIZE, 100])})\n",
    "    sess.run(generator_optimize, feed_dict={Z_in: get_sample_z([BATCH_SIZE, 100])})\n",
    "    \n",
    "    disc_losses = sess.run(discriminator_loss, feed_dict={Z_in: get_sample_z([BATCH_SIZE, 100]),\n",
    "                                                          image_in: image_batch})\n",
    "    gen_losses = sess.run(generator_loss, feed_dict={Z_in: get_sample_z([BATCH_SIZE, 100])})\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('Step {} => Discriminator: {} | Generator: {}'.format(i, disc_losses, gen_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_g = generator(Z_in, 1, True)\n",
    "x = sess.run(sample_g, feed_dict={Z_in: get_sample_z([1, 100])})\n",
    "display_image(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
